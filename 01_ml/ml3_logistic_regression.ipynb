{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML3. Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая функция хорошо приближает значения $(0, 1)$:\n",
    "\n",
    "$ logistic(z)=\\frac{P}{1+e^{-kz}} $\n",
    "\n",
    "На практике мы используем частный случай логистической функции — **сигмоида**:\n",
    "\n",
    "$ \\sigma (z)=\\frac{1}{1+e^{-z}} $\n",
    "\n",
    "Сигмоида принимает значения от 0 до 1. В контексте классификации это значение можно трактовать как вероятность того, что объект принадлежит одному из двух классов. Тогда **уравнение сигмоиды** для задачи логистической регрессии будет выглядеть так:\n",
    "\n",
    "$ \\sigma (x;\\beta )=\\frac{1}{1+e^{-\\dot{\\beta} x}} $\n",
    "\n",
    "Для нахождения **оптимального значения** параметров нужно решить следующую задачу оптимизации:\n",
    "\n",
    "$ min\\sum_{1}^{i}ln(1+e^{-y_i\\dot{\\beta }x}) $\n",
    "\n",
    "Решать её можно **градиентным спуском**.\n",
    "\n",
    "Когда мы получаем на выходе модели нули и единицы, мы можем использовать обычные **метрики** для классификации — *Accuracy, f1 score* и другие. Если мы хотим посчитать **ошибку для вероятностей**, которые возвращает сигмоида, можно воспользоваться *logloss*:\n",
    "\n",
    "$ logloss(y_{true}, y_{pred})=-y_{true}ln y_{pred}-(1-y_{true})ln(1-y_{pred}) $\n",
    "\n",
    "То же самое для нескольких классов:\n",
    "\n",
    "- $y_{true_-ij}$ - бинарная переменная. 1 — если пример *i* имеет класс *j*, иначе равно 0.\n",
    "- $y_{pred_-ij}$ - вероятность того, что пример *i* имеет класс *j*, берётся из модели.\n",
    "\n",
    "**Softmax** — обобщение сигмоиды для многомерного случая. Функция преобразует вектор *z* размерности в вектор той же размерности, где каждая координата полученного вектора представлена вещественным числом в интервале $[0,1]$ и сумма координат равна 1.\n",
    "\n",
    "$ \\sigma (z)_i=\\frac{e^{z_i}}{\\sum_{k=1}^{K}e^{z_k}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5.3\n",
    "\n",
    "Посчитайте *logloss* для данных в таблице (без нормализации). Укажите число с точностью до сотых:\n",
    "\n",
    "| Предсказанное значение | Истинное значение |\n",
    "|------------------------|-------------------|\n",
    "| 0.2                    | 0                 |\n",
    "| 0.8                    | 0                 |\n",
    "| 1                      | 1                 |\n",
    "| 0.6                    | 1                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_true, y_pred):\n",
    "    return -y_true * np.log(y_true) - (1 - y_true) * np.log(1 - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.343407087514302"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array([0.2, 0.8, 1, 0.6])\n",
    "y_test = np.array([0, 0, 1, 1])\n",
    "\n",
    "log_loss(y_test, y_pred) * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5.4\n",
    "\n",
    "Посчитайте *logloss* для данных в таблице. Необходимо найти среднюю ошибку. Классификация на три класса:\n",
    "\n",
    "| Предсказанное значение | Истинное значение |\n",
    "|------------------------|-------------------|\n",
    "| 0.2, 0.3, 0.5          | 0, 0, 1           |\n",
    "| 0, 0, 1                | 0, 0, 1           |\n",
    "| 0.1, 0, 0.9            | 1, 0, 0           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([\n",
    "    [0.2, 0.3, 0.5],\n",
    "    [0,     0,   1],\n",
    "    [0.1,   0, 0.9],\n",
    "])\n",
    "y_true = np.array([\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-07dbb1c021a6>:1: RuntimeWarning: divide by zero encountered in log\n",
      "  m = y_true * np.log(y_pred)\n",
      "<ipython-input-27-07dbb1c021a6>:1: RuntimeWarning: invalid value encountered in multiply\n",
      "  m = y_true * np.log(y_pred)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9985774245179969"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = y_true * np.log(y_pred)\n",
    "m_sum = m[~np.isnan(m)].sum()\n",
    "llm = -1 / len(y_true) * m_sum\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985774245179969"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length, width = y_pred.shape\n",
    "\n",
    "logloss_multy = 0\n",
    "for i in range(length):\n",
    "    for j in range(width):\n",
    "        if y_true[i,j] == y_pred[i,j]:\n",
    "            continue\n",
    "        else:\n",
    "            logloss_multy += y_true[i,j] * np.log(y_pred[i,j])\n",
    "logloss_multy *= -(1/length)\n",
    "logloss_multy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Логистическая регрессия. Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем логистическую регрессию. Начнём с импорта библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае с логистической регрессией мы можем использовать только **градиентный спуск**, так как нет явного матричного способа найти оптимальные коэффициенты. В качестве функции потерь будем использовать **бинарную кросс-энтропию**, *Log Loss*. Она записывается так:\n",
    "\n",
    "$ L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i))) $\n",
    "\n",
    "Градиент ошибки:\n",
    "\n",
    "$ \\nabla L=\\frac{1}{n}\\sum_{i=1}^{n}{(h_{\\theta}(x_i)-y_i)x_i} $\n",
    "\n",
    "Будем использовать другой датасет с задачей **классификации**, где нужно определить зарплату меньше и больше определённого значения. Убираем в данных лишние признаки, конвертируем целевой столбец в бинарные значения и нормализуем данные.\n",
    "\n",
    "Реализуем функцию `sigmoid` и функцию, вычисляющую **градиент бинарной кросс-энтропии:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, theta):\n",
    "    return 1. / (1. + np.exp(-X.dot(theta)))\n",
    "\n",
    "def calc_binary_cross_entropy_grad(X, y, theta):\n",
    "    n = X.shape[0]\n",
    "    grad = 1. / n * X.transpose().dot(sigmoid(X, theta) - y)\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def gradient_step(theta, theta_grad, alpha):\n",
    "    return theta - alpha * theta_grad\n",
    "def optimize(X, y, grad_func, start_theta, alpha, n_iters):\n",
    "    theta = start_theta.copy()\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        theta_grad = grad_func(X, y, theta)\n",
    "        theta = gradient_step(theta, theta_grad, alpha)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказания на тренировочной выборке и посчитаем значение метрики *accuracy* и *F1 score*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e9496e853657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint_logisitc_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = sigmoid(X, theta) > 0.5\n",
    "print_logisitc_metrics(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже знаем, что этой выборке нельзя доверять, поэтому разбиваем данные и оптимизируем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c0999f37749c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalc_binary_cross_entropy_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint_logisitc_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "theta = optimize(X_train, y_train, calc_binary_cross_entropy_grad, np.ones(m), 1., 300)\n",
    "y_pred = sigmoid(X_valid, theta) > 0.5\n",
    "\n",
    "print_logisitc_metrics(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат практически тот же. Отрисуем *ROC-кривую*, посчитаем её значения и значение площади под кривой *AUC*.\n",
    "\n",
    "Для борьбы с **переобучением** добавим **регуляризацию**. Обернём линейную регрессию в **класс:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegOptimizer():\n",
    "    def __init__(self, alpha, n_iters):\n",
    "        self.theta = None\n",
    "        self._alpha = alpha\n",
    "        self._n_iters = n_iters\n",
    "    \n",
    "    def gradient_step(self, theta, theta_grad):\n",
    "        return theta - self._alpha * theta_grad\n",
    "    \n",
    "    def grad_func(self, X, y, theta):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def optimize(self, X, y, start_theta, n_iters):\n",
    "        theta = start_theta.copy()\n",
    "\n",
    "        for _ in range(n_iters):\n",
    "            theta_grad = self.grad_func(X, y, theta)\n",
    "            theta = self.gradient_step(theta, theta_grad)\n",
    "\n",
    "        return theta\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m = X.shape[1]\n",
    "        start_theta = np.ones(m)\n",
    "        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проделаем ту же операцию с логистичекой регрессией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(RegOptimizer):\n",
    "    def sigmoid(self, X, theta):\n",
    "        return 1. / (1. + np.exp(-X.dot(theta)))\n",
    "    \n",
    "    def grad_func(self, X, y, theta):\n",
    "        n = X.shape[0]\n",
    "        grad = 1. / n * X.transpose().dot(self.sigmoid(X, theta) - y)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X, self.theta)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.theta is None:\n",
    "            raise Exception('You should train the model first')\n",
    "        \n",
    "        y_pred = self.predict_proba(X) > 0.5\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Избавлимся от лишних признаков, нормализуем данные. С переобучением боремся с помощью **регуляризации**.\n",
    "\n",
    "$ \\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2} $\n",
    "\n",
    "После добавления регуляризации функция ошибки **линейной регрессии** будет выглядеть так:\n",
    "\n",
    "$ L=\\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - \\theta^Tx_i)^2} + \\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2} $\n",
    "\n",
    "А её градиент по параметру $\\theta$:\n",
    "\n",
    "$ \\nabla L = \\frac{1}{n}\\sum_{i=1}^{n}{(\\theta^Tx_i - y_i) \\cdot x_i} + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j} = \\frac{1}{n}X^T(X\\theta - y) + \\frac{\\lambda}{m}\\sum_{j=1}^{m}{\\theta_j} $\n",
    "\n",
    "Функция ошибки для **логистической регрессии** в случае бинарной классификации с регуляризатором записывается так:\n",
    "\n",
    "$ L=-\\frac{1}{n}(y_i \\log h_{\\theta}(x_i) + (1-y_i) \\log(1-h_{\\theta}(x_i)))+\\frac{\\lambda}{2m}\\sum_{j}^{m}{\\theta_j^2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве экспериментальных данных возьмем датасет о доходах граждан в различных странах Adult Income (файл также можно скачать из [открытого источника](https://archive.ics.uci.edu/ml/datasets/Adult)). Далее сделаем необходимую предобработку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/adult.data', names=[\n",
    "    'age', 'workclass', 'fnlwgt', 'education',\n",
    "    'education-num', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'capital-gain',\n",
    "    'capital-loss', 'hours-per-week', 'native-country', 'salary',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  salary  \n",
       "0          2174             0              40       0  \n",
       "1             0             0              13       0  \n",
       "2             0             0              40       0  \n",
       "3             0             0              40       0  \n",
       "4             0             0              40       0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['salary'] = (df['salary'] != ' <=50K').astype('int32')\n",
    "df.drop('native-country', axis=1, inplace=True)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "features = df[cols].values\n",
    "mu = features.mean(axis=0)\n",
    "sigma = features.std(axis=0)\n",
    "norm_features = (features - mu) / sigma\n",
    "\n",
    "df.loc[:, cols] = norm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>salary</th>\n",
       "      <th>workclass_ ?</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_ Own-child</th>\n",
       "      <th>relationship_ Unmarried</th>\n",
       "      <th>relationship_ Wife</th>\n",
       "      <th>race_ Amer-Indian-Eskimo</th>\n",
       "      <th>race_ Asian-Pac-Islander</th>\n",
       "      <th>race_ Black</th>\n",
       "      <th>race_ Other</th>\n",
       "      <th>race_ White</th>\n",
       "      <th>sex_ Female</th>\n",
       "      <th>sex_ Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030671</td>\n",
       "      <td>-1.063611</td>\n",
       "      <td>1.134739</td>\n",
       "      <td>0.148453</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.837109</td>\n",
       "      <td>-1.008707</td>\n",
       "      <td>1.134739</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-2.222153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.042642</td>\n",
       "      <td>0.245079</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.057047</td>\n",
       "      <td>0.425801</td>\n",
       "      <td>-1.197459</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.775768</td>\n",
       "      <td>1.408176</td>\n",
       "      <td>1.134739</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>-0.849080</td>\n",
       "      <td>0.639741</td>\n",
       "      <td>0.746039</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.197409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>0.103983</td>\n",
       "      <td>-0.335433</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>1.423610</td>\n",
       "      <td>-0.358777</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>-1.215643</td>\n",
       "      <td>0.110960</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-1.655225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>0.983734</td>\n",
       "      <td>0.929893</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>1.888424</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "0      0.030671 -1.063611       1.134739      0.148453      -0.21666   \n",
       "1      0.837109 -1.008707       1.134739     -0.145920      -0.21666   \n",
       "2     -0.042642  0.245079      -0.420060     -0.145920      -0.21666   \n",
       "3      1.057047  0.425801      -1.197459     -0.145920      -0.21666   \n",
       "4     -0.775768  1.408176       1.134739     -0.145920      -0.21666   \n",
       "...         ...       ...            ...           ...           ...   \n",
       "32556 -0.849080  0.639741       0.746039     -0.145920      -0.21666   \n",
       "32557  0.103983 -0.335433      -0.420060     -0.145920      -0.21666   \n",
       "32558  1.423610 -0.358777      -0.420060     -0.145920      -0.21666   \n",
       "32559 -1.215643  0.110960      -0.420060     -0.145920      -0.21666   \n",
       "32560  0.983734  0.929893      -0.420060      1.888424      -0.21666   \n",
       "\n",
       "       hours-per-week  salary  workclass_ ?  workclass_ Federal-gov  \\\n",
       "0           -0.035429       0             0                       0   \n",
       "1           -2.222153       0             0                       0   \n",
       "2           -0.035429       0             0                       0   \n",
       "3           -0.035429       0             0                       0   \n",
       "4           -0.035429       0             0                       0   \n",
       "...               ...     ...           ...                     ...   \n",
       "32556       -0.197409       0             0                       0   \n",
       "32557       -0.035429       1             0                       0   \n",
       "32558       -0.035429       0             0                       0   \n",
       "32559       -1.655225       0             0                       0   \n",
       "32560       -0.035429       1             0                       0   \n",
       "\n",
       "       workclass_ Local-gov  ...  relationship_ Own-child  \\\n",
       "0                         0  ...                        0   \n",
       "1                         0  ...                        0   \n",
       "2                         0  ...                        0   \n",
       "3                         0  ...                        0   \n",
       "4                         0  ...                        0   \n",
       "...                     ...  ...                      ...   \n",
       "32556                     0  ...                        0   \n",
       "32557                     0  ...                        0   \n",
       "32558                     0  ...                        0   \n",
       "32559                     0  ...                        1   \n",
       "32560                     0  ...                        0   \n",
       "\n",
       "       relationship_ Unmarried  relationship_ Wife  race_ Amer-Indian-Eskimo  \\\n",
       "0                            0                   0                         0   \n",
       "1                            0                   0                         0   \n",
       "2                            0                   0                         0   \n",
       "3                            0                   0                         0   \n",
       "4                            0                   1                         0   \n",
       "...                        ...                 ...                       ...   \n",
       "32556                        0                   1                         0   \n",
       "32557                        0                   0                         0   \n",
       "32558                        1                   0                         0   \n",
       "32559                        0                   0                         0   \n",
       "32560                        0                   1                         0   \n",
       "\n",
       "       race_ Asian-Pac-Islander  race_ Black  race_ Other  race_ White  \\\n",
       "0                             0            0            0            1   \n",
       "1                             0            0            0            1   \n",
       "2                             0            0            0            1   \n",
       "3                             0            1            0            0   \n",
       "4                             0            1            0            0   \n",
       "...                         ...          ...          ...          ...   \n",
       "32556                         0            0            0            1   \n",
       "32557                         0            0            0            1   \n",
       "32558                         0            0            0            1   \n",
       "32559                         0            0            0            1   \n",
       "32560                         0            0            0            1   \n",
       "\n",
       "       sex_ Female  sex_ Male  \n",
       "0                0          1  \n",
       "1                0          1  \n",
       "2                0          1  \n",
       "3                0          1  \n",
       "4                1          0  \n",
       "...            ...        ...  \n",
       "32556            1          0  \n",
       "32557            0          1  \n",
       "32558            1          0  \n",
       "32559            0          1  \n",
       "32560            1          0  \n",
       "\n",
       "[32561 rows x 67 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.get_dummies(df, columns=[\n",
    "    'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6.1\n",
    "\n",
    "Постройте модель логистической регрессии при помощи `sklearn`. Используйте параметры по умолчанию, обучите на всей выборке и посчитайте *F1 score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.drop('salary', axis=1).values\n",
    "x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "y = df_train['salary'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6616594131686088"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = LogisticRegression(max_iter=100)\n",
    "solver.fit(x, y)\n",
    "\n",
    "y_pred = solver.predict(x)\n",
    "\n",
    "f1_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6.2\n",
    "\n",
    "Посчитайте *confusion matrix* для классификатора из задачи 3.6.1. Для получения матрицы можно воспользоваться методом `sklearn.metrics.confusion_matrix(y_true, y_pred)`, либо посчитать каждый элемент вручную.\n",
    "\n",
    "|            | 1 (predicted)  | 0 (predicted)  |\n",
    "|------------|----------------|----------------|\n",
    "| 1 (actual) | True Positive  | False Negative |\n",
    "| 0 (actual) | False Positive | True Negative  |\n",
    "\n",
    "Введите значения получившейся матрицы в соответствующие ячейки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23028,  1692],\n",
       "       [ 3128,  4713]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = confusion_matrix(y, y_pred)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6.3\n",
    "\n",
    "Постройте *ROC-кривую* и посчитайте $ROC-AUC$ для классификатора из задачи 3.6.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отрисовать ROC кривую\n",
    "def calc_and_plot_roc(y_true, y_pred_proba):\n",
    "    # Посчитать значения ROC кривой и значение площади под кривой AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    plt.title('Receiver Operating Characteristic', fontsize=15)\n",
    "    plt.xlabel('False positive rate (FPR)', fontsize=15)\n",
    "    plt.ylabel('True positive rate (TPR)', fontsize=15)\n",
    "    plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "calc_and_plot_roc(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6.4\n",
    "\n",
    "Переберите коэффициенты *l2-регуляризации* от 0.01 до 1 с шагом 0.01 и определите, на каком из них модель логистической регрессии из *sklearn* даёт наибольший *F1 score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train.drop('salary', axis=1).values\n",
    "x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "y = df_train['salary'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
       "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
       "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33,\n",
       "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
       "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
       "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
       "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
       "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
       "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0.01, 1, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for c in np.arange(0.01, 1, 0.01):\n",
    "    solver = LogisticRegression(solver='newton-cg', penalty='l2', C=c)\n",
    "    solver.fit(x, y)\n",
    "    y_pred = solver.predict(x)\n",
    "    score = f1_score(y, y_pred)\n",
    "    scores.append((c, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.65, 0.6624087591240876)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores, key=lambda x: x[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6.5\n",
    "\n",
    "Замените в столбце `native-country` страны, у которых меньше ста записей, на `other`, поменяйте этот столбец на *dummy-переменные*, обучите классификатор на всей выборке и посчитайте *F1 score*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/adult.data', names=[\n",
    "    'age', 'workclass', 'fnlwgt', 'education',\n",
    "    'education-num', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'sex', 'capital-gain',\n",
    "    'capital-loss', 'hours-per-week', 'native-country', 'salary',\n",
    "])\n",
    "\n",
    "df['salary'] = (df['salary'] != ' <=50K').astype('int32')\n",
    "\n",
    "cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "features = df[cols].values\n",
    "mu = features.mean(axis=0)\n",
    "sigma = features.std(axis=0)\n",
    "norm_features = (features - mu) / sigma\n",
    "df.loc[:, cols] = norm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = df['native-country'].value_counts()\n",
    "x = country_counts[country_counts < 100]\n",
    "df['native-country'] = df['native-country'].replace(x.index, 'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>salary</th>\n",
       "      <th>native-country_ ?</th>\n",
       "      <th>native-country_ Canada</th>\n",
       "      <th>native-country_ El-Salvador</th>\n",
       "      <th>...</th>\n",
       "      <th>relationship_ Own-child</th>\n",
       "      <th>relationship_ Unmarried</th>\n",
       "      <th>relationship_ Wife</th>\n",
       "      <th>race_ Amer-Indian-Eskimo</th>\n",
       "      <th>race_ Asian-Pac-Islander</th>\n",
       "      <th>race_ Black</th>\n",
       "      <th>race_ Other</th>\n",
       "      <th>race_ White</th>\n",
       "      <th>sex_ Female</th>\n",
       "      <th>sex_ Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030671</td>\n",
       "      <td>-1.063611</td>\n",
       "      <td>1.134739</td>\n",
       "      <td>0.148453</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.837109</td>\n",
       "      <td>-1.008707</td>\n",
       "      <td>1.134739</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-2.222153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.042642</td>\n",
       "      <td>0.245079</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.057047</td>\n",
       "      <td>0.425801</td>\n",
       "      <td>-1.197459</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.775768</td>\n",
       "      <td>1.408176</td>\n",
       "      <td>1.134739</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>-0.849080</td>\n",
       "      <td>0.639741</td>\n",
       "      <td>0.746039</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.197409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>0.103983</td>\n",
       "      <td>-0.335433</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>1.423610</td>\n",
       "      <td>-0.358777</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>-1.215643</td>\n",
       "      <td>0.110960</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>-0.145920</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-1.655225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>0.983734</td>\n",
       "      <td>0.929893</td>\n",
       "      <td>-0.420060</td>\n",
       "      <td>1.888424</td>\n",
       "      <td>-0.21666</td>\n",
       "      <td>-0.035429</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "0      0.030671 -1.063611       1.134739      0.148453      -0.21666   \n",
       "1      0.837109 -1.008707       1.134739     -0.145920      -0.21666   \n",
       "2     -0.042642  0.245079      -0.420060     -0.145920      -0.21666   \n",
       "3      1.057047  0.425801      -1.197459     -0.145920      -0.21666   \n",
       "4     -0.775768  1.408176       1.134739     -0.145920      -0.21666   \n",
       "...         ...       ...            ...           ...           ...   \n",
       "32556 -0.849080  0.639741       0.746039     -0.145920      -0.21666   \n",
       "32557  0.103983 -0.335433      -0.420060     -0.145920      -0.21666   \n",
       "32558  1.423610 -0.358777      -0.420060     -0.145920      -0.21666   \n",
       "32559 -1.215643  0.110960      -0.420060     -0.145920      -0.21666   \n",
       "32560  0.983734  0.929893      -0.420060      1.888424      -0.21666   \n",
       "\n",
       "       hours-per-week  salary  native-country_ ?  native-country_ Canada  \\\n",
       "0           -0.035429       0                  0                       0   \n",
       "1           -2.222153       0                  0                       0   \n",
       "2           -0.035429       0                  0                       0   \n",
       "3           -0.035429       0                  0                       0   \n",
       "4           -0.035429       0                  0                       0   \n",
       "...               ...     ...                ...                     ...   \n",
       "32556       -0.197409       0                  0                       0   \n",
       "32557       -0.035429       1                  0                       0   \n",
       "32558       -0.035429       0                  0                       0   \n",
       "32559       -1.655225       0                  0                       0   \n",
       "32560       -0.035429       1                  0                       0   \n",
       "\n",
       "       native-country_ El-Salvador  ...  relationship_ Own-child  \\\n",
       "0                                0  ...                        0   \n",
       "1                                0  ...                        0   \n",
       "2                                0  ...                        0   \n",
       "3                                0  ...                        0   \n",
       "4                                0  ...                        0   \n",
       "...                            ...  ...                      ...   \n",
       "32556                            0  ...                        0   \n",
       "32557                            0  ...                        0   \n",
       "32558                            0  ...                        0   \n",
       "32559                            0  ...                        1   \n",
       "32560                            0  ...                        0   \n",
       "\n",
       "       relationship_ Unmarried  relationship_ Wife  race_ Amer-Indian-Eskimo  \\\n",
       "0                            0                   0                         0   \n",
       "1                            0                   0                         0   \n",
       "2                            0                   0                         0   \n",
       "3                            0                   0                         0   \n",
       "4                            0                   1                         0   \n",
       "...                        ...                 ...                       ...   \n",
       "32556                        0                   1                         0   \n",
       "32557                        0                   0                         0   \n",
       "32558                        1                   0                         0   \n",
       "32559                        0                   0                         0   \n",
       "32560                        0                   1                         0   \n",
       "\n",
       "       race_ Asian-Pac-Islander  race_ Black  race_ Other  race_ White  \\\n",
       "0                             0            0            0            1   \n",
       "1                             0            0            0            1   \n",
       "2                             0            0            0            1   \n",
       "3                             0            1            0            0   \n",
       "4                             0            1            0            0   \n",
       "...                         ...          ...          ...          ...   \n",
       "32556                         0            0            0            1   \n",
       "32557                         0            0            0            1   \n",
       "32558                         0            0            0            1   \n",
       "32559                         0            0            0            1   \n",
       "32560                         0            0            0            1   \n",
       "\n",
       "       sex_ Female  sex_ Male  \n",
       "0                0          1  \n",
       "1                0          1  \n",
       "2                0          1  \n",
       "3                0          1  \n",
       "4                1          0  \n",
       "...            ...        ...  \n",
       "32556            1          0  \n",
       "32557            0          1  \n",
       "32558            1          0  \n",
       "32559            0          1  \n",
       "32560            1          0  \n",
       "\n",
       "[32561 rows x 77 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.get_dummies(df, columns=[\n",
    "    'native-country', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex'\n",
    "])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6632051641874824"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df_train.drop('salary', axis=1).values\n",
    "x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "y = df_train['salary'].values\n",
    "\n",
    "solver = LogisticRegression(solver='newton-cg')\n",
    "solver.fit(x, y)\n",
    "\n",
    "y_pred = solver.predict(x)\n",
    "f1_score(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
